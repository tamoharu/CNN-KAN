{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN-KANを用いた画像分類**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolmogorov-Arnold Network (KAN) の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KANLinearクラスの定義\n",
    "KANLinearは、KANの核となる線形変換層である。この層は、通常の線形変換に加えてB-スプライン基底関数を用いた非線形変換を組み合わせている。\n",
    "KANLinearクラスは、入力特徴量を非線形変換し、出力特徴量を生成する。主な特徴は以下の通り：\n",
    "- B-スプライン基底関数を用いた非線形変換\n",
    "- グリッド更新機能\n",
    "- 正則化損失の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__`メソッドは、KANLinear層の初期化を行う。主な役割は以下の通り：\n",
    "- グリッドの初期化\n",
    "- 基本重みとスプライン重みのパラメータ化\n",
    "- 各種スケーリングパラメータの設定\n",
    "- 活性化関数の設定\n",
    "\n",
    "`b_splines`メソッドは、入力テンソルに対してB-スプライン基底を計算する。これにより、非線形変換の基礎となる基底関数が得られる。\n",
    "\n",
    "`curve2coeff`メソッドは、与えられた点を補間する曲線の係数を計算する。これにより、スプライン重みが更新される。\n",
    "\n",
    "`forward`メソッドは、入力テンソルに対して順伝播を行う。基本的な線形変換と、B-スプライン基底を用いた非線形変換を組み合わせて出力を生成する。\n",
    "\n",
    "`update_grid`メソッドは、入力データの分布に基づいてグリッドを更新する。これにより、モデルが入力データの特性に適応できる。\n",
    "\n",
    "`regularization_lossメ`ソッドは、モデルの過学習を防ぐための正則化損失を計算する。活性化の正則化とエントロピーの正則化を組み合わせている。\n",
    "\n",
    "これらのメソッドにより、KANLinear層は非線形システムの近似に適した柔軟な変換を行うことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features), requires_grad=False)\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        与えられた入力テンソルに対してB-スプライン基底を計算する。\n",
    "\n",
    "        引数:\n",
    "            x (torch.Tensor): 形状(batch_size, in_features)の入力テンソル。\n",
    "            y (torch.Tensor): 形状(batch_size, in_features, out_features)の出力テンソル。\n",
    "\n",
    "        戻り値:\n",
    "            torch.Tensor: 形状(batch_size, in_features, grid_size + spline_order)のB-スプライン基底テンソル。\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.size(-1) == self.in_features\n",
    "        original_shape = x.shape\n",
    "        x = x.reshape(-1, self.in_features)\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        output = base_output + spline_output\n",
    "        \n",
    "        output = output.reshape(*original_shape[:-1], self.out_features)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        正則化損失を計算する。\n",
    "\n",
    "        これは、論文で述べられているオリジナルのL1正則化のシミュレーションである。\n",
    "        オリジナルの方法では、F.linear関数の背後に隠れている中間テンソル(batch, in_features, out_features)から\n",
    "        絶対値とエントロピーを計算する必要があるが、メモリ効率の良い実装を目指すため、この方法を採用している。\n",
    "\n",
    "        L1正則化は現在、スプライン重みの平均絶対値として計算されている。\n",
    "        著者の実装では、サンプルベースの正則化に加えてこの項も含まれている。\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KANクラスの定義\n",
    "KANクラスでは、複数のKANLinear層を組み合わせて全体のネットワークを構成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__`メソッドは、KANネットワーク全体の初期化を行う。主な役割は以下の通り：\n",
    "- グリッドサイズとスプライン次数の設定\n",
    "- 隠れ層の構造に基づいてKANLinear層のリストを作成\n",
    "- 各KANLinear層に共通のハイパーパラメータを設定\n",
    "\n",
    "`layers_hidden`引数は、ネットワークの各層のユニット数を指定するリストである。例えば、[64, 32, 16]と指定すると、入力層64ユニット、第1隠れ層32ユニット、出力層16ユニットのネットワークが構築される。\n",
    "\n",
    "`forward`メソッドは、入力テンソルに対してネットワーク全体の順伝播を行う。主な特徴は以下の通り：\n",
    "- 各KANLinear層を順番に適用\n",
    "- オプションで各層のグリッドを更新可能（update_grid=Trueの場合）\n",
    "- 最終的な出力を返す\n",
    "\n",
    "`update_grid`オプションを使用することで、モデルを入力データの分布に適応させることができる。これは、特に分布が時間とともに変化する動的システムのモデリングに有用である。\n",
    "\n",
    "`regularization_loss`メソッドは、ネットワーク全体の正則化損失を計算する。主な特徴は以下の通り：\n",
    "- 各KANLinear層の正則化損失を合計\n",
    "- 活性化の正則化とエントロピーの正則化を調整可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_hidden,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KAN, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n",
    "            self.layers.append(\n",
    "                KANLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    grid_size=grid_size,\n",
    "                    spline_order=spline_order,\n",
    "                    scale_noise=scale_noise,\n",
    "                    scale_base=scale_base,\n",
    "                    scale_spline=scale_spline,\n",
    "                    base_activation=base_activation,\n",
    "                    grid_eps=grid_eps,\n",
    "                    grid_range=grid_range,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, update_grid=False):\n",
    "        for layer in self.layers:\n",
    "            if update_grid:\n",
    "                layer.update_grid(x)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        return sum(\n",
    "            layer.regularization_loss(regularize_activation, regularize_entropy)\n",
    "            for layer in self.layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNの実装\n",
    "このCNNモデルの構造は以下のようになっている：\n",
    "\n",
    "1. **畳み込み部分**:\n",
    "- 3つの畳み込み層 (conv1, conv2, conv3)\n",
    "- 2つの最大プーリング層\n",
    "- 特徴マップのサイズを徐々に小さくしながら、特徴を抽出\n",
    "\n",
    "\n",
    "2. **全結合部分**:\n",
    "- 3つの全結合層 (fc1, fc2, fc3)\n",
    "- 畳み込み部分の出力を平坦化し、最終的な分類を行う\n",
    "\n",
    "3. **正則化**:\n",
    "- 2つのドロップアウト層 (畳み込み後と全結合層間)\n",
    "\n",
    "4. **活性化関数**:\n",
    "- 指定された活性化関数 (デフォルトはSiLU) を各層で使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,  # (channels, height, width)\n",
    "                 base_channels=32, \n",
    "                 hidden_units=64, \n",
    "                 dropout1_rate=0.25,\n",
    "                 dropout2_rate=0.5,\n",
    "                 base_activation=torch.nn.SiLU):\n",
    "        super(CNN, self).__init__()\n",
    "        input_channels, height, width = input_size\n",
    "        self.base_activation = base_activation()\n",
    "        self.conv1 = torch.nn.Conv2d(input_channels, base_channels, 3, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(base_channels, base_channels * 2, 3, 1)\n",
    "        self.conv3 = torch.nn.Conv2d(base_channels * 2, base_channels, 1)\n",
    "        conv_output_size = self._get_conv_output_size(height, width, base_channels)\n",
    "        self.fc1 = torch.nn.Linear(conv_output_size, hidden_units)\n",
    "        self.fc2 = torch.nn.Linear(hidden_units, hidden_units // 2)\n",
    "        self.fc3 = torch.nn.Linear(hidden_units // 2, 10)\n",
    "        self.dropout1 = torch.nn.Dropout2d(dropout1_rate)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout2_rate)\n",
    "\n",
    "    def _get_conv_output_size(self, height, width, base_channels):\n",
    "        size_h = (height - 2) // 2\n",
    "        size_h = (size_h - 2) // 2\n",
    "        size_w = (width - 2) // 2\n",
    "        size_w = (size_w - 2) // 2\n",
    "        return size_h * size_w * base_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_activation(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.base_activation(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.base_activation(self.conv3(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.base_activation(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.base_activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-KANの実装\n",
    "CNNの2つの全結合層を削除し、KAN層を追加している。\n",
    "一つ目の全結合層はKAN層の次元削減のための役割を果たしている。これにより、学習時間が削減される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNKAN(CNN):\n",
    "    def __init__(self, \n",
    "                 input_size,  # (channels, height, width)\n",
    "                 base_channels=32, \n",
    "                 kan_hidden=128,\n",
    "                 dropout1_rate=0.25,\n",
    "                 dropout2_rate=0.5,\n",
    "                 base_activation=torch.nn.SiLU):\n",
    "        super(CNNKAN, self).__init__(input_size, base_channels, kan_hidden, dropout1_rate, dropout2_rate, base_activation)\n",
    "        _, height, width = input_size\n",
    "        conv_output_size = self._get_conv_output_size(height, width, base_channels)\n",
    "        del self.fc1\n",
    "        del self.fc2\n",
    "        del self.fc3\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(conv_output_size, kan_hidden)\n",
    "        self.kan = KAN([kan_hidden, kan_hidden // 4, 10])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_activation(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.base_activation(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.base_activation(self.conv3(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.base_activation(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.kan(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングコードの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットのロード\n",
    "\n",
    "`load_mnist(batch_size=64)`\n",
    "\n",
    "MNISTデータセットを読み込み、データローダーを生成する。\n",
    "- 画像を正規化し、テンソルに変換\n",
    "- 訓練用、検証用、評価用のデータローダーを作成\n",
    "- バッチサイズは指定可能（デフォルト64）\n",
    "\n",
    "`load_cifar10(batch_size=64)`\n",
    "\n",
    "CIFAR-10データセットを読み込み、データローダーを生成する。\n",
    "- MNISTと同様の処理を行うが、CIFAR-10用の正規化パラメータを使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`load_cifar10(batch_size=64)`\n",
    "\n",
    "CIFAR-10データセットを読み込み、データローダーを生成する。\n",
    "- MNISTと同様の処理を行うが、CIFAR-10用の正規化パラメータを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    valset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "    eval_loader = DataLoader(valset, batch_size=len(valset), shuffle=False)\n",
    "    return trainloader, valloader, eval_loader\n",
    "\n",
    "def load_cifar10(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "    eval_loader = DataLoader(valset, batch_size=len(valset), shuffle=False)\n",
    "    return trainloader, valloader, eval_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのセットアップ\n",
    "\n",
    "`setup_model(model)`\n",
    "\n",
    "モデルのセットアップを行う。\n",
    "- GPUが利用可能な場合はGPUを使用\n",
    "- AdamWオプティマイザと学習率スケジューラを設定\n",
    "- 損失関数としてクロスエントロピーを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4) # type: ignore\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    return model, optimizer, scheduler, criterion, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チェックポイントの保存・ロード\n",
    "\n",
    "`save_checkpoint(model, optimizer, scheduler, epoch, val_accuracy, filename)`\n",
    "\n",
    "モデルの状態をチェックポイントとして保存する。\n",
    "\n",
    "`load_checkpoint(filename, model, optimizer, scheduler)`\n",
    "\n",
    "保存されたチェックポイントからモデルの状態を読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, val_accuracy, filename):\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.makedirs('checkpoints')\n",
    "    file_path = os.path.join('checkpoints', filename)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_accuracy': val_accuracy\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer, scheduler):\n",
    "    file_path = os.path.join('checkpoints', filename)\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練・評価関数\n",
    "\n",
    "`train_epoch(model, trainloader, optimizer, criterion, device)`\n",
    "\n",
    "1エポックの訓練を行う。\n",
    "- tqdmを使用して進捗を表示\n",
    "- 各バッチごとに損失を計算し、モデルを更新\n",
    "\n",
    "`validate(model, valloader, criterion, device)`\n",
    "\n",
    "検証データセットでモデルの性能を評価する。\n",
    "\n",
    "`evaluate(model, dataloader, device)`\n",
    "\n",
    "指定されたデータローダーを使用してモデルの精度を評価する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, trainloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    with tqdm(trainloader, desc=\"Training\") as pbar:\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(model, KAN):\n",
    "                images = images.view(images.size(0), -1)\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = (output.argmax(dim=1) == labels).float().mean()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", accuracy=f\"{accuracy.item():.4f}\")\n",
    "    return loss.item(), accuracy.item() # type: ignore\n",
    "\n",
    "def validate(model, valloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if isinstance(model, KAN):\n",
    "                images = images.view(images.size(0), -1)\n",
    "            output = model(images)\n",
    "            val_loss += criterion(output, labels).item()\n",
    "            val_accuracy += (output.argmax(dim=1) == labels).float().mean().item()\n",
    "    return val_loss / len(valloader), val_accuracy / len(valloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            if isinstance(model, KAN):\n",
    "                images = images.view(images.size(0), -1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_comparison(results, dataset, epochs)`\n",
    "\n",
    "複数のモデルの訓練結果を比較するグラフを作成し保存する。\n",
    "\n",
    "トレーニング時と検証時に分けてグラフを作成する。\n",
    "\n",
    "1. 損失比較プロット（左）\n",
    "- 各モデルの検証損失を表示\n",
    "\n",
    "2. 精度比較プロット（右）\n",
    "- 各モデルの検証精度を表示\n",
    "\n",
    "3. グラフの調整と保存\n",
    "- 軸ラベル、タイトル、凡例を設定\n",
    "- 'figures'ディレクトリに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(results, dataset, epochs):\n",
    "    if not os.path.exists('figures'):\n",
    "        os.makedirs('figures')\n",
    "\n",
    "    file_path_train = os.path.join('figures', f'{dataset}_{epochs}_training_comparison.png')\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model, data in results.items():\n",
    "        plt.plot(np.arange(1, epochs + 1), data['train_losses'], label=f'{model} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss Comparison ({dataset})')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model, data in results.items():\n",
    "        plt.plot(np.arange(1, epochs + 1), data['train_accuracies'], label=f'{model} Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Training Accuracy Comparison ({dataset})')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path_train)\n",
    "    plt.close()\n",
    "\n",
    "    file_path_val = os.path.join('figures', f'{dataset}_{epochs}_validation_comparison.png')\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model, data in results.items():\n",
    "        plt.plot(np.arange(1, epochs + 1), data['val_losses'], label=f'{model} Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Validation Loss Comparison ({dataset})')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model, data in results.items():\n",
    "        plt.plot(np.arange(1, epochs + 1), data['val_accuracies'], label=f'{model} Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Validation Accuracy Comparison ({dataset})')\n",
    "    plt.legend()\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path_val)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main関数\n",
    "\n",
    "モデルの訓練・評価を行う。\n",
    "\n",
    "1. データセットの選択と読み込み\n",
    "- 'mnist' または 'cifar10' から選択\n",
    "- 選択されたデータセットに応じてデータローダーを生成\n",
    "\n",
    "1. モデルの選択と初期化\n",
    "- 'cnn'、'kan'、'cnn_kan' から選択\n",
    "- 選択されたモデルを初期化し、入力チャンネル数を設定\n",
    "\n",
    "3. モデルのセットアップ\n",
    "- デバイス（GPU/CPU）の設定\n",
    "- オプティマイザ、スケジューラ、損失関数の初期化\n",
    "\n",
    "4. 訓練ループ\n",
    "- 指定されたエポック数だけ訓練を繰り返す\n",
    "- 各エポックで訓練と検証を実行\n",
    "- 学習率のスケジューリングを行う\n",
    "- 訓練と検証の損失と精度を表示\n",
    "\n",
    "5. ベストモデルの保存\n",
    "- 検証精度が最高値を更新するたびにモデルを保存\n",
    "\n",
    "6. ベストモデルの評価\n",
    "- 保存された最良のモデルを読み込む\n",
    "- 訓練データセット全体での精度を評価\n",
    "- テストデータセット全体での精度を評価\n",
    "\n",
    "7. 結果の可視化\n",
    "- 全モデルの訓練・検証結果を比較するグラフを生成\n",
    "- 損失と精度の推移を別々のサブプロットで表示\n",
    "- 結果を画像ファイルとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, models, epochs):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    if dataset == 'mnist':\n",
    "        trainloader, valloader, eval_loader = load_mnist()\n",
    "        input_size = (1, 28, 28)\n",
    "    elif dataset == 'cifar10':\n",
    "        trainloader, valloader, eval_loader = load_cifar10()\n",
    "        input_size = (3, 32, 32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for modelname in models:\n",
    "        print(f\"\\nTraining {modelname} on {dataset}\")\n",
    "        if modelname == 'cnn':\n",
    "            model = CNN(input_size)\n",
    "        elif modelname == 'kan':\n",
    "            input_size_flat = input_size[0] * input_size[1] * input_size[2]\n",
    "            model = KAN([input_size_flat, 64, 10])\n",
    "        elif modelname == 'cnn-kan':\n",
    "            model = CNNKAN(input_size)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {modelname}\")\n",
    "\n",
    "        model, optimizer, scheduler, criterion, device = setup_model(model)\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        best_val_accuracy = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        filename = f'{modelname}_{dataset}_{epochs}_checkpoint.pth'\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = train_epoch(model, trainloader, optimizer, criterion, device)\n",
    "            val_loss, val_accuracy = validate(model, valloader, criterion, device)\n",
    "            scheduler.step()\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Epoch {epoch + 1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, val_accuracy, filename)\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f\"Total training time for {modelname}: {total_time:.2f} seconds\")\n",
    "\n",
    "        best_epoch, best_accuracy = load_checkpoint(filename, model, optimizer, scheduler)\n",
    "        print(f\"Loaded best model from epoch {best_epoch+1} with validation accuracy {best_accuracy:.4f}\")\n",
    "\n",
    "        test_accuracy = evaluate(model, eval_loader, device)\n",
    "        print(f\"Test Accuracy for {modelname}: {test_accuracy:.4f}\")\n",
    "\n",
    "        results[modelname] = {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'test_accuracy': test_accuracy\n",
    "        }\n",
    "\n",
    "    plot_comparison(results, dataset, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニング実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(dataset='mnist', models=['cnn', 'kan', 'cnn-kan'], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(dataset='cifar10', models=['cnn', 'kan', 'cnn-kan'], epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
